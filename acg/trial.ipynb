{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "import copy\n",
    "import os\n",
    "from config import config\n",
    "\n",
    "IGNORE_INDEX=-100\n",
    "\n",
    "class ACGDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,root_dir, window = 15, fps = 1,\n",
    "                 tokenizer_name = config.model.language_model.tokenizer_name, max_token_length=128):\n",
    "        \n",
    "        self.root_path = root_dir  \n",
    "        self.vid_embs_dir = os.path.join(self.root_path, \"vid_embs\")\n",
    "        self.commentary_dir = os.path.join(self.root_path, \"standardized_transcripts_filtered\")\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.window = window \n",
    "        \n",
    "        # the video embeddings are sampled at two embedding per second. \n",
    "        #need to hcange the below parameter as req\n",
    "        self.fps = fps\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,use_auth_token=True)\n",
    "        self.tokenizer.pad_token_id = 128001\n",
    "        self.tokenizer.add_tokens([\"[BATSMAN]\",\"[BOWLER]\", \"[FIELDER]\", \n",
    "                                   \"[UMPIRE]\", \"[VENUE]\" ], special_tokens=True)\n",
    "        self.max_token_length = max_token_length\n",
    "        \n",
    "        self.vid_ids = self._get_valid_vidids()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        vid_id = self.vid_ids[idx]\n",
    "        \n",
    "        #load npy file\n",
    "        features_path = os.path.join(self.vid_embs_dir, vid_id+\"_embeddings.npy\")\n",
    "        features = np.load(features_path) # Time, Patches(256+cls), Dimension\n",
    "        features = self._resample_features(features,self.fps)\n",
    "        \n",
    "        \n",
    "        #load commentary for hte vid file\n",
    "        commentary = self._get_vid_commentary(vid_id)\n",
    "        \n",
    "        tokens = self.tokenizer(commentary,return_tensors=\"pt\", \n",
    "                                max_length=self.max_token_length,truncation=True).input_ids[0]\n",
    "        \n",
    "    \n",
    "        return {'vid_features': features, \"tokens\": tokens, \"commentary\": commentary}\n",
    "  \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.vid_ids)\n",
    "    \n",
    "    def _resample_features(self, features, fps, time_step=0.5):\n",
    "        # Calculate the new time interval between samples\n",
    "        new_interval = time_step / fps  # How often to sample (in seconds)\n",
    "        \n",
    "        # Calculate the index step (how many original time steps to skip)\n",
    "        step = int(1 / new_interval)\n",
    "        \n",
    "        # Create an array of indices for resampling based on fps\n",
    "        new_time_indices = np.arange(0, features.shape[0], step)\n",
    "        \n",
    "        # Return the features sampled at the new time indices\n",
    "        return features[new_time_indices]\n",
    "    \n",
    "    def _get_vid_commentary(self, vid_id):\n",
    "        \"\"\"\n",
    "        Reads and returns the text from the commentary file associated with the given vid_id.\n",
    "\n",
    "        Args:\n",
    "            vid_id (str): The video ID whose commentary is to be fetched.\n",
    "\n",
    "        Returns:\n",
    "            str: The content of the commentary file.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the .txt file for the given vid_id does not exist.\n",
    "        \"\"\"\n",
    "        \n",
    "        file_path = os.path.join(self.commentary_dir, f\"{vid_id}.txt\")\n",
    "        \n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Commentary file not found for vid_id: {vid_id}\")\n",
    "\n",
    "        \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "    \n",
    "\n",
    "\n",
    "    def _get_valid_vidids(self):\n",
    "        \"\"\"\n",
    "        Finds video IDs (vidids) in the given directory for which both .txt and _embeddings.npy files are present.\n",
    "\n",
    "        Args:\n",
    "            directory (str): Path to the directory containing the files.\n",
    "\n",
    "        Returns:\n",
    "            list: List of vidids with both .txt and _embeddings.npy files present.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Get the list of files in both directories\n",
    "        vid_files = os.listdir(self.vid_embs_dir)\n",
    "        com_files = os.listdir(self.commentary_dir)\n",
    "        \n",
    "        # Get the base filenames without extensions for .npy files\n",
    "        emb_files = {os.path.splitext(f)[0].split('_embeddings')[0] for f in vid_files if f.endswith('_embeddings.npy')}\n",
    "        \n",
    "        # Get the base filenames without extensions for .txt files\n",
    "        com_files = {os.path.splitext(f)[0] for f in com_files if f.endswith('.txt')}\n",
    "        \n",
    "      \n",
    "        # Find the intersection of the two sets\n",
    "        valid_vidids = emb_files.intersection(com_files)\n",
    "        \n",
    "        \n",
    "        return list(valid_vidids)\n",
    "    \n",
    "    \n",
    "    def collator(self,batch):\n",
    "        \n",
    "        out_batch= {}\n",
    "\n",
    "        input_ids = [\n",
    "            torch.cat((torch.tensor([self.tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\")]),\n",
    "                        instance[\"tokens\"],\n",
    "                        torch.tensor([self.tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\")]))) for instance in batch] \n",
    "\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        \n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels,\n",
    "            batch_first=True,\n",
    "            padding_value=IGNORE_INDEX)\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"))\n",
    "\n",
    "        attention_mask=input_ids.ne(self.tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"))\n",
    "        \n",
    "        vid_features = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.from_numpy(instance['vid_features']) for instance in batch], batch_first=True )\n",
    "        \n",
    "        commentaries = [instance['commentary'] for instance in batch]\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "        # if 'vid_features' in batch[0]:\n",
    "        #     features = [torch.from_numpy(instance['vid_features']) for instance in batch]\n",
    "        # if all(x is not None and x.shape == features[0].shape for x in features):\n",
    "        #     out_batch['vid_features'] = torch.stack(features)\n",
    "        # else:\n",
    "        #     out_batch['vid_features'] = features\n",
    "        \n",
    "        \n",
    "            \n",
    "        out_batch['vid_features'] = vid_features\n",
    "        out_batch['input_ids']=input_ids\n",
    "        out_batch['attention_mask'] =attention_mask\n",
    "        out_batch['labels']=labels\n",
    "        out_batch['commentary'] = commentaries\n",
    "        return out_batch\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = ACGDataset(\"/home/hoffman/Documents/UT/Stuffs/Applied ML/project/data/pre_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [ds[4], ds[6], ds[43]]\n",
    "\n",
    "collated_batch =  ds.collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "import contextlib\n",
    "from Qformer import BertConfig, BertLMHeadModel\n",
    "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
    "from typing import List\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from config import config \n",
    "\n",
    "\n",
    "def process_output_tokens(predict_model, tokens):\n",
    "    output_texts = []\n",
    "    for output_token in tokens:\n",
    "        output_text = predict_model.tokenizer.decode(output_token)\n",
    "        end_token_index = output_text.find('<|end_of_text|>')\n",
    "        if end_token_index != -1:\n",
    "            output_text = output_text[:end_token_index]\n",
    "        output_texts.append(output_text)\n",
    "    return output_texts\n",
    "    \n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "        \n",
    "\n",
    "class OPTModel(nn.Module):\n",
    "    def __init__(self, max_frame_pos=128, \n",
    "                 window=30, num_query_tokens=32, \n",
    "                 num_video_query_token=32, num_features=1024, \n",
    "                 device = \"cuda\", inference=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.model.language_model.tokenizer_name)\n",
    "        #self.tokenizer.add_tokens([\"[PLAYER]\",\"[TEAM]\",\"([TEAM])\"], special_tokens=True)\n",
    "        self.opt_model = AutoModelForCausalLM.from_pretrained(config.model.language_model.llm_name, torch_dtype=torch.bfloat16)\n",
    "        #self.opt_model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.eos_token_id = self.tokenizer(\n",
    "            \"\\n\", add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        #self.llama_model.parallelize()\n",
    "        \n",
    "        self.ln_vision = LayerNorm(num_features)\n",
    "        self.num_query_tokens = num_query_tokens,\n",
    "        self.num_video_query_token = num_video_query_token\n",
    "        self.inference = inference\n",
    "        \n",
    "        \n",
    "         # Initialize video Q-former\n",
    "        self.video_Qformer,self.video_query_tokens = self.init_video_Qformer(num_query_token = num_video_query_token,\n",
    "                                                                             vision_width=num_features,\n",
    "                                                                             num_hidden_layers =2)\n",
    "        self.video_Qformer.cls = None\n",
    "        self.video_Qformer.bert.embeddings.word_embeddings = None\n",
    "        self.video_Qformer.bert.embeddings.position_embeddings = None\n",
    "        for layer in self.video_Qformer.bert.encoder.layer:\n",
    "            layer.output = None\n",
    "            layer.intermediate = None\n",
    "\n",
    "        # llama projection\n",
    "        self.opt_proj = nn.Linear(\n",
    "            self.video_Qformer.config.hidden_size, 512\n",
    "        )\n",
    "        # video frame positional embedding\n",
    "        self.video_frame_position_embedding = nn.Embedding(max_frame_pos, num_features)\n",
    "        self.window = window\n",
    "\n",
    "        # move to device\n",
    "        self.opt_model = self.opt_model.to(self.device)\n",
    "        for name, param in self.opt_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.video_Qformer = self.video_Qformer.to(self.device)\n",
    "        self.opt_proj = self.opt_proj.to(self.device)\n",
    "        self.ln_vision = self.ln_vision.to(self.device)\n",
    "        for name, param in self.ln_vision.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.ln_vision = self.ln_vision.eval()\n",
    "        self.video_frame_position_embedding = self.video_frame_position_embedding.to(self.device)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def init_video_Qformer(cls, num_query_token, vision_width, num_hidden_layers =2):\n",
    "        encoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        encoder_config.num_hidden_layers = num_hidden_layers\n",
    "        encoder_config.encoder_width = vision_width\n",
    "        # insert cross-attention layer every other block\n",
    "        encoder_config.add_cross_attention = True\n",
    "        encoder_config.cross_attention_freq = 1\n",
    "        encoder_config.query_length = num_query_token\n",
    "        Qformer = BertLMHeadModel(config=encoder_config)\n",
    "        query_tokens = nn.Parameter(\n",
    "            torch.zeros(1, num_query_token, encoder_config.hidden_size)\n",
    "        )\n",
    "        query_tokens.data.normal_(mean=0.0, std=encoder_config.initializer_range)\n",
    "        return Qformer, query_tokens\n",
    "    \n",
    "    def forward(self, batch, validating=False):\n",
    "        \n",
    "        video_features = batch['vid_features'].to(self.device) #B,T,P,D or [T,P,D]\n",
    "        input_ids= batch['input_ids']#Bxmax(T)\n",
    "        atts_opt = batch['attention_mask']  #Bxmax(T)\n",
    "        targets= batch['labels'] #Bxmax(T)\n",
    "        \n",
    "\n",
    "            \n",
    "        batch_size, time_length, _, _ = video_features.size()\n",
    "        \n",
    "        video_features = self.ln_vision(video_features)\n",
    "        \n",
    "        video_features = einops.rearrange(video_features, 'b t n f -> (b t) n f', b=batch_size, t=time_length)\n",
    "        position_ids = torch.arange(time_length, dtype=torch.long, device=video_features.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        frame_position_embeddings = self.video_frame_position_embedding(position_ids)\n",
    "        frame_position_embeddings = frame_position_embeddings.unsqueeze(-2)\n",
    "        \n",
    "        frame_hidden_state = einops.rearrange(video_features, '(b t) n f -> b t n f',b=batch_size,t=time_length)\n",
    "        frame_hidden_state = frame_position_embeddings + frame_hidden_state\n",
    "        \n",
    "        frame_hidden_state =  einops.rearrange(frame_hidden_state, 'b t q h -> b (t q) h',b=batch_size,t=time_length)\n",
    "        \n",
    "        \n",
    "        frame_atts = torch.ones(frame_hidden_state.size()[:-1], dtype=torch.long).to(frame_hidden_state)\n",
    "        \n",
    "        video_query_tokens = self.video_query_tokens.expand(frame_hidden_state.shape[0], -1, -1).to(frame_hidden_state.device)\n",
    "        \n",
    "        video_query_output = self.video_Qformer.bert(\n",
    "            query_embeds=video_query_tokens,\n",
    "            encoder_hidden_states=frame_hidden_state,\n",
    "            encoder_attention_mask=frame_atts,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        video_hidden = video_query_output.last_hidden_state\n",
    "        \n",
    "        inputs_opt = self.opt_proj(video_hidden)\n",
    "        #inputs_embeds = self.opt_model.model.decoder.embed_tokens(input_ids)\n",
    "        \n",
    "        if self.inference:\n",
    "            return self.generate_text(inputs_opt)\n",
    "        \n",
    "        if validating:\n",
    "            response = self.generate_text(inputs_opt)\n",
    "            return response, batch['commentary']\n",
    "        \n",
    "        #atts_llama\n",
    "        \n",
    "        visual_label = torch.full((batch_size, self.num_video_query_token), -100, dtype=targets.dtype)\n",
    "        concat_targets = torch.cat((visual_label, targets), dim=1).to(self.device)\n",
    "        temp_input_ids = input_ids.clone().to(self.device)\n",
    "        targets_embeds = self.opt_model.model.decoder.embed_tokens(temp_input_ids)\n",
    "        \n",
    "        embedding_cat = torch.cat((inputs_opt, targets_embeds), dim=1)\n",
    "        mask_prefix = torch.ones(batch_size, self.num_video_query_token, dtype=atts_opt.dtype)\n",
    "        mask = torch.concat((mask_prefix, atts_opt), dim=1).to(self.device)\n",
    "        \n",
    "        \n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = io.StringIO()\n",
    "        \n",
    "        with self.maybe_autocast():\n",
    "            outputs = self.opt_model(\n",
    "                inputs_embeds=embedding_cat,\n",
    "                attention_mask=mask,\n",
    "                return_dict=True,\n",
    "                labels=concat_targets,\n",
    "            )\n",
    "            \n",
    "        sys.stdout = original_stdout\n",
    "        loss = outputs.loss\n",
    "        return loss\n",
    "    \n",
    "\n",
    "        \n",
    "    # def generate_text(self, inputs_opt):\n",
    "    #     start_embeds = self.opt_model.model.decoder.embed_tokens(torch.tensor([128000]).to(self.device))\n",
    "    #     inputs_llama_with_s = torch.cat([inputs_opt, start_embeds.expand(inputs_opt.size(0), -1, -1)], dim=1).to(dtype=torch.bfloat16)\n",
    "    #     temp_res_tokens = self.opt_model.generate(\n",
    "    #         renormalize_logits=True,\n",
    "    #         inputs_embeds=inputs_llama_with_s,\n",
    "    #         max_new_tokens=128,\n",
    "    #         num_beams=5,\n",
    "    #         do_sample=True,\n",
    "    #         min_length=5,\n",
    "    #         top_p=0.9,\n",
    "    #         repetition_penalty=1.0,\n",
    "    #         length_penalty=1,\n",
    "    #         temperature=1.0,\n",
    "    #     )\n",
    "    #     res_text = process_output_tokens(self, temp_res_tokens)\n",
    "    #     return res_text\n",
    "        \n",
    "        \n",
    "    \n",
    "    def maybe_autocast(self, dtype=torch.float16):\n",
    "        enable_autocast = self.device != torch.device(\"cpu\")\n",
    "        if enable_autocast:\n",
    "            return torch.cuda.amp.autocast(dtype=dtype)\n",
    "        else:\n",
    "            return contextlib.nullcontext()\n",
    "        \n",
    "        \n",
    "    def generate_text(self, inputs_opt):\n",
    "        start_embeds = self.opt_model.model.embed_tokens(torch.tensor([128000]).to(self.device))\n",
    "        # inputs_opt_with_s = torch.cat([inputs_opt, start_embeds.expand(inputs_opt.size(0), -1, -1)], dim=1).to(dtype=torch.bfloat16)\n",
    "        # temp_res_tokens = self.opt_model.generate(\n",
    "        #     renormalize_logits=True,\n",
    "        #     inputs_embeds=inputs_opt_with_s,\n",
    "        #     max_new_tokens=128,\n",
    "        #     num_beams=5,\n",
    "        #     do_sample=True,\n",
    "        #     min_length=5,\n",
    "        #     top_p=0.9,\n",
    "        #     repetition_penalty=1.0,\n",
    "        #     length_penalty=1,\n",
    "        #     temperature=1.0,\n",
    "        # )\n",
    "        # res_text = process_output_tokens(self, temp_res_tokens)\n",
    "        # return res_text\n",
    "        return None\n",
    "\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2705187739.py, line 234)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[44], line 234\u001b[0;36m\u001b[0m\n\u001b[0;31m    output_text = self.opt_tokenizer.batch_decode(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "import contextlib\n",
    "from Qformer import BertConfig, BertLMHeadModel\n",
    "from transformers.generation.logits_process import LogitsProcessor, LogitsProcessorList\n",
    "from typing import List\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from config import config \n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def process_output_tokens(predict_model, tokens):\n",
    "    output_texts = []\n",
    "    for output_token in tokens:\n",
    "        output_text = predict_model.tokenizer.decode(output_token)\n",
    "        end_token_index = output_text.find('<|end_of_text|>')\n",
    "        if end_token_index != -1:\n",
    "            output_text = output_text[:end_token_index]\n",
    "        output_texts.append(output_text)\n",
    "    return output_texts\n",
    "    \n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "        \n",
    "\n",
    "class OPTModel(nn.Module):\n",
    "    def __init__(self, max_frame_pos=128, \n",
    "                 window=30, num_query_tokens=32, \n",
    "                 num_video_query_token=32, num_features=1024, \n",
    "                 device = \"cuda\", inference=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.opt_tokenizer = AutoTokenizer.from_pretrained(config.model.language_model.tokenizer_name)\n",
    "        #self.opt_tokenizer.add_tokens([\"[PLAYER]\",\"[TEAM]\",\"([TEAM])\"], special_tokens=True)\n",
    "        self.opt_model = AutoModelForCausalLM.from_pretrained(config.model.language_model.llm_name, torch_dtype=torch.bfloat16)\n",
    "        #self.opt_model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", truncation_side=truncation_side)\n",
    "        tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n",
    "        \n",
    "        self.eos_token_id = self.opt_tokenizer(\n",
    "            \"\\n\", add_special_tokens=False\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        #self.llama_model.parallelize()\n",
    "        \n",
    "        self.ln_vision = LayerNorm(num_features)\n",
    "        self.num_query_tokens = num_query_tokens,\n",
    "        self.num_video_query_token = num_video_query_token\n",
    "        self.inference = inference\n",
    "        \n",
    "        \n",
    "         # Initialize video Q-former\n",
    "        self.video_Qformer,self.video_query_tokens = self.init_video_Qformer(num_query_token = num_video_query_token,\n",
    "                                                                             vision_width=num_features,\n",
    "                                                                             num_hidden_layers =2)\n",
    "        self.video_Qformer.cls = None\n",
    "        self.video_Qformer.bert.embeddings.word_embeddings = None\n",
    "        self.video_Qformer.bert.embeddings.position_embeddings = None\n",
    "        for layer in self.video_Qformer.bert.encoder.layer:\n",
    "            layer.output = None\n",
    "            layer.intermediate = None\n",
    "\n",
    "        # llama projection\n",
    "        self.opt_proj = nn.Linear(\n",
    "            self.video_Qformer.config.hidden_size, self.opt_model.config.hidden_size\n",
    "        )\n",
    "        # video frame positional embedding\n",
    "        self.video_frame_position_embedding = nn.Embedding(max_frame_pos, num_features)\n",
    "        self.window = window\n",
    "\n",
    "        # move to device\n",
    "        self.opt_model = self.opt_model.to(self.device)\n",
    "        for name, param in self.opt_model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.video_Qformer = self.video_Qformer.to(self.device)\n",
    "        self.opt_proj = self.opt_proj.to(self.device)\n",
    "        self.ln_vision = self.ln_vision.to(self.device)\n",
    "        for name, param in self.ln_vision.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.ln_vision = self.ln_vision.eval()\n",
    "        self.video_frame_position_embedding = self.video_frame_position_embedding.to(self.device)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def init_video_Qformer(cls, num_query_token, vision_width, num_hidden_layers =2):\n",
    "        encoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        encoder_config.num_hidden_layers = num_hidden_layers\n",
    "        encoder_config.encoder_width = vision_width\n",
    "        # insert cross-attention layer every other block\n",
    "        encoder_config.add_cross_attention = True\n",
    "        encoder_config.cross_attention_freq = 1\n",
    "        encoder_config.query_length = num_query_token\n",
    "        Qformer = BertLMHeadModel(config=encoder_config)\n",
    "        query_tokens = nn.Parameter(\n",
    "            torch.zeros(1, num_query_token, encoder_config.hidden_size)\n",
    "        )\n",
    "        query_tokens.data.normal_(mean=0.0, std=encoder_config.initializer_range)\n",
    "        return Qformer, query_tokens\n",
    "    \n",
    "    def forward(self, batch, validating=False):\n",
    "        \n",
    "        video_features = batch['vid_features'].to(self.device) #B,T,P,D or [T,P,D]\n",
    "        input_ids= batch['input_ids']#Bxmax(T)\n",
    "        atts_opt = batch['attention_mask']  #Bxmax(T)\n",
    "        targets= batch['labels'] #Bxmax(T)\n",
    "        commentary = batch['commentary']\n",
    "\n",
    "            \n",
    "        batch_size, time_length, _, _ = video_features.size()\n",
    "        \n",
    "        video_features = self.ln_vision(video_features)\n",
    "        \n",
    "        video_features = einops.rearrange(video_features, 'b t n f -> (b t) n f', b=batch_size, t=time_length)\n",
    "        position_ids = torch.arange(time_length, dtype=torch.long, device=video_features.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        frame_position_embeddings = self.video_frame_position_embedding(position_ids)\n",
    "        frame_position_embeddings = frame_position_embeddings.unsqueeze(-2)\n",
    "        \n",
    "        frame_hidden_state = einops.rearrange(video_features, '(b t) n f -> b t n f',b=batch_size,t=time_length)\n",
    "        frame_hidden_state = frame_position_embeddings + frame_hidden_state\n",
    "        \n",
    "        frame_hidden_state =  einops.rearrange(frame_hidden_state, 'b t q h -> b (t q) h',b=batch_size,t=time_length)\n",
    "        \n",
    "        \n",
    "        frame_atts = torch.ones(frame_hidden_state.size()[:-1], dtype=torch.long).to(frame_hidden_state)\n",
    "        \n",
    "        video_query_tokens = self.video_query_tokens.expand(frame_hidden_state.shape[0], -1, -1).to(frame_hidden_state.device)\n",
    "        \n",
    "        video_query_output = self.video_Qformer.bert(\n",
    "            query_embeds=video_query_tokens,\n",
    "            encoder_hidden_states=frame_hidden_state,\n",
    "            encoder_attention_mask=frame_atts,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        \n",
    "        video_hidden = video_query_output.last_hidden_state\n",
    "        \n",
    "        inputs_opt = self.opt_proj(video_hidden)\n",
    "        #inputs_embeds = self.opt_model.model.decoder.embed_tokens(input_ids)\n",
    "        \n",
    "        # targets = opt_tokens.input_ids.masked_fill(\n",
    "        #     input_ids == self.opt_tokenizer.pad_token_id, -100\n",
    "        # ).to(self.device)\n",
    "        \n",
    "        # empty_targets = (\n",
    "        #     torch.ones(atts_opt.size(), dtype=torch.long).to(self.device).fill_(-100)\n",
    "        # )\n",
    "        \n",
    "        # targets = torch.cat([empty_targets, targets], dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.inference:\n",
    "            return self.generate_text(inputs_opt)\n",
    "        \n",
    "        if validating:\n",
    "            response = self.generate_text(inputs_opt)\n",
    "            return response, batch['commentary']\n",
    "        \n",
    "        #atts_llama\n",
    "        \n",
    "        visual_label = torch.full((batch_size, self.num_video_query_token), -100, dtype=targets.dtype)\n",
    "        concat_targets = torch.cat((visual_label, targets), dim=1).to(self.device)\n",
    "        \n",
    "        temp_input_ids = input_ids.clone().to(self.device)\n",
    "        targets_embeds = self.opt_model.model.decoder.embed_tokens(temp_input_ids)\n",
    "        \n",
    "        embedding_cat = torch.cat((inputs_opt, targets_embeds), dim=1)\n",
    "        mask_prefix = torch.ones(batch_size, self.num_video_query_token, dtype=atts_opt.dtype)\n",
    "        mask = torch.concat((mask_prefix, atts_opt), dim=1).to(self.device)\n",
    "        \n",
    "        \n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = io.StringIO()\n",
    "        \n",
    "        with self.maybe_autocast():\n",
    "            outputs = self.opt_model(\n",
    "                inputs_embeds=embedding_cat,\n",
    "                attention_mask=mask,\n",
    "                return_dict=True,\n",
    "                labels=concat_targets,\n",
    "            )\n",
    "            \n",
    "        sys.stdout = original_stdout\n",
    "        loss = outputs.loss\n",
    "        return loss\n",
    "    \n",
    "\n",
    "        \n",
    "    def generate_text(self, inputs_opt,\n",
    "        use_nucleus_sampling=False,\n",
    "        num_beams=5,\n",
    "        max_length=30,\n",
    "        min_length=1,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        num_captions=1,\n",
    "        temperature=1,):\n",
    "        \n",
    "        batch_size = inputs_opt.shape[0]\n",
    "        mask_prefix = torch.ones(batch_size, self.num_video_query_token, dtype=atts_opt.dtype).to(self.device)\n",
    "        \n",
    "        outputs = self.opt_model.generate(\n",
    "                inputs_embeds=inputs_opt, \n",
    "                attention_mask=mask_prefix,\n",
    "                do_sample=use_nucleus_sampling,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                num_beams=num_beams,\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                length_penalty=length_penalty,\n",
    "                num_return_sequences=num_captions,\n",
    "            )\n",
    "        \n",
    "         output_text = self.opt_tokenizer.batch_decode(\n",
    "                outputs, skip_special_tokens=True\n",
    "            )\n",
    "         \n",
    "         output_text = [text.strip() for text in output_text]\n",
    "         return output_text\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    \n",
    "    def maybe_autocast(self, dtype=torch.float16):\n",
    "        enable_autocast = self.device != torch.device(\"cpu\")\n",
    "        if enable_autocast:\n",
    "            return torch.cuda.amp.autocast(dtype=dtype)\n",
    "        else:\n",
    "            return contextlib.nullcontext()\n",
    "        \n",
    "        \n",
    "    # def generate_text(self, inputs_opt):\n",
    "    #     start_embeds = self.opt_model.model.embed_tokens(torch.tensor([128000]).to(self.device))\n",
    "    #     # inputs_opt_with_s = torch.cat([inputs_opt, start_embeds.expand(inputs_opt.size(0), -1, -1)], dim=1).to(dtype=torch.bfloat16)\n",
    "    #     # temp_res_tokens = self.opt_model.generate(\n",
    "    #     #     renormalize_logits=True,\n",
    "    #     #     inputs_embeds=inputs_opt_with_s,\n",
    "    #     #     max_new_tokens=128,\n",
    "    #     #     num_beams=5,\n",
    "    #     #     do_sample=True,\n",
    "    #     #     min_length=5,\n",
    "    #     #     top_p=0.9,\n",
    "    #     #     repetition_penalty=1.0,\n",
    "    #     #     length_penalty=1,\n",
    "    #     #     temperature=1.0,\n",
    "    #     # )\n",
    "    #     # res_text = process_output_tokens(self, temp_res_tokens)\n",
    "    #     # return res_text\n",
    "    #     return None\n",
    "\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_features = collated_batch['vid_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 22, 257, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, time_length, _, _ = video_features.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ne() received an invalid combination of arguments - got (torch.Size, dtype=torch.dtype), but expected one of:\n * (Tensor input, Tensor other, *, Tensor out = None)\n * (Tensor input, Number other, *, Tensor out = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m frame_hidden_state \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(video_features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(b t) n f -> b t n f\u001b[39m\u001b[38;5;124m'\u001b[39m,b\u001b[38;5;241m=\u001b[39mbatch_size,t\u001b[38;5;241m=\u001b[39mtime_length)\n\u001b[1;32m      7\u001b[0m frame_hidden_state \u001b[38;5;241m=\u001b[39m  einops\u001b[38;5;241m.\u001b[39mrearrange(frame_hidden_state, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb t q h -> b (t q) h\u001b[39m\u001b[38;5;124m'\u001b[39m,b\u001b[38;5;241m=\u001b[39mbatch_size,t\u001b[38;5;241m=\u001b[39mtime_length)\n\u001b[0;32m---> 10\u001b[0m frame_atts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(frame_hidden_state)\n",
      "\u001b[0;31mTypeError\u001b[0m: ne() received an invalid combination of arguments - got (torch.Size, dtype=torch.dtype), but expected one of:\n * (Tensor input, Tensor other, *, Tensor out = None)\n * (Tensor input, Number other, *, Tensor out = None)\n"
     ]
    }
   ],
   "source": [
    "video_features = einops.rearrange(video_features, 'b t n f -> (b t) n f', b=batch_size, t=time_length)\n",
    "\n",
    "\n",
    "\n",
    "frame_hidden_state = einops.rearrange(video_features, '(b t) n f -> b t n f',b=batch_size,t=time_length)\n",
    "\n",
    "frame_hidden_state =  einops.rearrange(frame_hidden_state, 'b t q h -> b (t q) h',b=batch_size,t=time_length)\n",
    "\n",
    "\n",
    "frame_atts = torch.ones(frame_hidden_state.size()[:-1], dtype=torch.long).to(frame_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5654, 1024])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5654])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_atts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model  \u001b[38;5;241m=\u001b[39m \u001b[43mOPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39m model(collated_batch, validating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[18], line 79\u001b[0m, in \u001b[0;36mOPTModel.__init__\u001b[0;34m(self, max_frame_pos, window, num_query_tokens, num_video_query_token, num_features, device, inference)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m window\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# move to device\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     81\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/transformers/modeling_utils.py:2065\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2061\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2062\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2063\u001b[0m     )\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model  = OPTModel()\n",
    "\n",
    "out = model(collated_batch, validating=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50268. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3341, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20068/4199484760.py:210: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return torch.cuda.amp.autocast(dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.4282, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = model(collated_batch)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model  \u001b[38;5;241m=\u001b[39m \u001b[43mOPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m out \u001b[38;5;241m=\u001b[39m model(collated_batch, validating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[16], line 79\u001b[0m, in \u001b[0;36mOPTModel.__init__\u001b[0;34m(self, max_frame_pos, window, num_query_tokens, num_video_query_token, num_features, device, inference)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m window\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# move to device\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     81\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/transformers/modeling_utils.py:2065\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2061\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2062\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2063\u001b[0m     )\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/hoffman/anaconda3/envs/thesis/lib/python3.11/site-packages/transformers/modeling_utils.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50268. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.cuda.FloatTensor' as parameter 'video_query_tokens' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMyModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_frame_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_query_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_video_query_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Change to \"cpu\" if no GPU is available\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform a forward pass\u001b[39;00m\n\u001b[1;32m     13\u001b[0m out \u001b[38;5;241m=\u001b[39m model(collated_batch)\n",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m, in \u001b[0;36mMyModel.__init__\u001b[0;34m(self, max_frame_pos, window, num_query_tokens, num_video_query_token, num_features, device, inference)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_Qformer\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Move all components to device\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 111\u001b[0m, in \u001b[0;36mMyModel._move_to_device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_move_to_device_p()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_Qformer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_Qformer\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_query_tokens\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_query_tokens\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_proj\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1750\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1751\u001b[0m                         )\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.cuda.FloatTensor' as parameter 'video_query_tokens' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from dataset import ACGDataset\n",
    "\n",
    "ds = ACGDataset(config.data.train_path)\n",
    "\n",
    "batch = [ds[0], ds[1], ds[3], ds[4]]\n",
    "\n",
    "collated_batch = ds.collator(batch)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MyModel(\n",
    "    max_frame_pos=128,\n",
    "    window=30,\n",
    "    num_query_tokens=32,\n",
    "    num_video_query_token=32,\n",
    "    num_features=1024,\n",
    "    device=\"cuda\",  # Change to \"cpu\" if no GPU is available\n",
    "    inference=False\n",
    ")\n",
    "\n",
    "# Perform a forward pass\n",
    "out = model(collated_batch)\n",
    "\n",
    "# Print the output\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
